{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA-Vodafone_Masterclass_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chantalskye/example/blob/master/CA_Vodafone_Masterclass_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaRtfShC0C97",
        "colab_type": "text"
      },
      "source": [
        "# Coder Academy - Vodafone Masterclass\n",
        "\n",
        "This notebook has the following sections...\n",
        "\n",
        "* Part 0: Problem introduction\n",
        "* Part 1: Using this notebook\n",
        "* Part 2: Playing with time series\n",
        "* Part 3: Visualising time series data\n",
        "* Part 4: Classifying time series data\n",
        "* Part 5: Detecting changes to the network environment\n",
        "\n",
        "You can pull up the **Table of contents** on the left hand-side in colab to skip around the notebook. We also **strongly** recommend going to your **Runtime->Change Runtime Type** and switching the **Hardware Accelerator** to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3lYAv1m0C99",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Problem introduction\n",
        "\n",
        "---\n",
        "\n",
        "Vodafone collects network data to be able gauge how well a network is performing towards expectation. By constantly analysing performance, Vodafone can be more **_proactive_** instead of **_reactive_** to respond to environmental changes that might affect their mobile network. \n",
        "\n",
        "<img src=\"https://d2bs8hqp6qvsw6.cloudfront.net/article/images/800x800/dimg/vodafone_24.jpg\" width=\"500\">\n",
        "\n",
        "\n",
        "## Scenario introduction\n",
        "\n",
        "Today we'll analyse the following scenario. Imagine a cell tower with two antennas, one that serves commuter traffic from a nearby train station, and another antenna that serves customers in an apartment building within close proximity to the cell tower.\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Problem_Scenario.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "There's a specific mobile traffic pattern that is distinct to an antenna that serves commuter vs. residential traffic. We can graph what's called a **time series** to look at the average daily traffic pattern for each of these antennas.\n",
        "\n",
        "---\n",
        "<img src=\"https://raw.githubusercontent.com/CoderAcademyEdu/data_science_sc_student/master/img/Problem_Scenario_w_Graphs.png\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "The question becomes, if we can **teach a computer to recognise the _normal traffic pattern_ for each of these antennas, will a computer subsequently be able to detect when mobile traffic _differs_ from the normal pattern?** Consider the following scenario, where _new_ apartment buildings are built in between the railway station and the antenna serving commuters...\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Problem_Scenario_w_new_apt.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "This environmental change might affect the line of sight, or the direct path from an antenna to a customer. If Vodafone can **detect the effects of the environmental changes**, it can make the necessary cell tower adjustments and respond to these changes, continuing a seamless customer experience.\n",
        "\n",
        "### Thought exercise\n",
        "\n",
        "Take 10-15 minutes and work with the partner next to you and research how we can use data science to build a network monitoring system. A few questions that might be relevant...\n",
        "\n",
        "* What is a time series?\n",
        "* How can a computer learn about patterns within a time series?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwqwoUn0C9-",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Using this notebook\n",
        "---\n",
        "\n",
        "## What is Python?\n",
        "\n",
        "Python is an _interpretive_ programming language invented in the 1980s. It's actually named after Monty Python and Holy Grail. In this class we'll be using Python to build our machine learning algorithms. \n",
        "\n",
        "### Why learn Python?\n",
        "\n",
        "Python has gained popularity because it has an easier syntax (rules to follow while coding) than many other programming languages. Python is very diverse in its applications which has led to its adoption in areas such as data science and web development.\n",
        "\n",
        "All of the following companies actively use Python:\n",
        "\n",
        "![Image](https://www.probytes.net/wp-content/uploads/2018/08/appl.png)\n",
        "\n",
        "## How do I interact with this notebook?\n",
        "\n",
        "A Jupyter Notebook is an interactive way to work with code in a web browser. Jupyter is a pseudo-acronym for three programming languages: Julia, python and (e)r. Notebooks provide a format to add instructions + code in one file, which is why we're using it!\n",
        "\n",
        "We'll quickly do some practice to introduce you how to use this notebook. For a list of keyboard shortcuts you can take a look at [Max Melnick's](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html) beginner tips for Jupyter Notebook.\n",
        "\n",
        "Here's a quick run down of some of the most basic commands to use:\n",
        "\n",
        "- A cell with a **<span style=\"color:blue\">blue</span>** background is in **Command Mode**. This will allow you to toggle up/down cells using the arrow keys. You can press enter/return on a cell in command mode to enter edit mode\n",
        "\n",
        "- A cell with a **<span style=\"color:green\">green</span>** background is in **Edit Mode**. This will allow you to change the content of cells. You can press the escape key on a cell in command mode to enter edit mode\n",
        "\n",
        "- To run the contents of a cell, you can type:\n",
        "  - `cmd + enter`, which will run the cotents of a cell and keep the cursor in place\n",
        "  - `shift + enter`, which will run the contents of a cell, and move the cursor to the next cell (or create a new cell)\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Edit the below by changing \"Gretchen\" to your own name by entering edit mode, and then running the cell using the directions above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuNTDq510C9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Hello, Gretchen!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0L8u12_0C-B",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Playing with time series\n",
        "\n",
        "---\n",
        "\n",
        "## Opening time series data\n",
        "\n",
        "Let's dive in by opening up a dataset that we'll use throughout the entire masterclass. This dataset represents simulated network loads across multiple antennas. To open up our dataset and manipulate it, we'll use the [pandas, or pan(el)-da(ta) + s](https://pandas.pydata.org/) library. We need to `import` the pandas library into our Python session to be able to use it.\n",
        "\n",
        "Run the following code (using `shift + enter`) to\n",
        "\n",
        "* import pandas\n",
        "* upload our dataset\n",
        "* print out some information about the data including...\n",
        "  * The number of rows\n",
        "  * The **data types** (do columns contain numbers or words?) within each column, as well as the column names\n",
        "  * The first five rows of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr5dYUqw0C-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read the data\n",
        "timeseries_data = pd.read_csv('https://raw.githubusercontent.com/CoderAcademyEdu/data_science_sc_student/master/data/traffic_workshop_data.csv')\n",
        "\n",
        "# Print the shape of the data\n",
        "print('This dataset has %d rows and %d columns \\n' % (timeseries_data.shape[0], timeseries_data.shape[1]))\n",
        "\n",
        "# Data types \n",
        "print('Column names and data types')\n",
        "print(timeseries_data.dtypes)\n",
        "\n",
        "# Print out the first five rows\n",
        "timeseries_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pzQdGjo0C-E",
        "colab_type": "text"
      },
      "source": [
        "Each row in the data represents a captured network load, from a specific antenna, at a specific time. There are four columns within the data. We can summarise them here...\n",
        "\n",
        "| Column name | Description |\n",
        "| ----------- | ----------- |\n",
        "| ANTENNA | The ID for a specific antenna. In this dataset, we have two antennas, labeled with ID's `0` and `1` |\n",
        "| SERVICE | The type of service the antenna is intended to cover. In our data, antenna `0` is expected to cover commuter service, representing the <span style=\"color:green;\">**green**</span> antenna, while the service for antenna `1` is residential, representing the <span style=\"color:red;\">**red**</span> antenna|\n",
        "| TIMESTAMP | The date and time when the specific load was recorded |\n",
        "| LOAD | The current **number of users** of the mobile network at the specific timestamp |\n",
        "\n",
        "Because this is timeseries data, it might be helpful to also learn a little bit about _when_ recordings took place. For instance...\n",
        "\n",
        "* Over what time period do we have data for each cell?\n",
        "* On average, how many records were recorded per cell, per day?\n",
        "* Do these records typically span the entire day, or only certain sections of the day?\n",
        "\n",
        "To do this, it will be easier to convert the `TIMESTAMP` column to a `datetime` object, and then we can extract specific details about the time each record occurred. We can use the [`pd.to_datetime()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) function to do this extraction.\n",
        "\n",
        "Run the code below to convert the timestamp to a datetime object, and then extract specific information around the timestamp.\n",
        "\n",
        "**Side note:** If you're interested in why we convert the `TIMESTAMP` to a `datetime` object, it's because it will allow us to use a bunch of internal Python capabilities and manipulations of dates. It's like someone giving you a PDF instead of a word document...it's a lot easier to change the text in a word document than it is a PDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBrZq5FO0C-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to datetime\n",
        "timeseries_data['TIMESTAMP'] = pd.to_datetime(timeseries_data['TIMESTAMP'])\n",
        "\n",
        "# Extract hours month, day of year, hour, day of year, weeknumber, year \n",
        "timeseries_data['MONTH'] = [d.month for d in timeseries_data['TIMESTAMP']]\n",
        "timeseries_data['HOUR'] = [d.hour for d in timeseries_data['TIMESTAMP']]\n",
        "timeseries_data['DAY'] = [d.dayofyear for d in timeseries_data['TIMESTAMP']]\n",
        "timeseries_data['DAYOFWEEK'] = [d.dayofweek for d in timeseries_data['TIMESTAMP']]\n",
        "timeseries_data['WEEKNUMBER'] = [d.isocalendar()[1] for d in timeseries_data['TIMESTAMP']]\n",
        "timeseries_data['YEAR'] = [d.year for d in timeseries_data['TIMESTAMP']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2dadkN0C-H",
        "colab_type": "text"
      },
      "source": [
        "Now we can extract the metrics listed above and learn more about our dataset. You'll see a lot of usage of the [pandas groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) function, which allows us to summarise data by different categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7KOlh-j0C-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract min and max date per cell\n",
        "print('********************************************************************')\n",
        "print('Min/max per antenna:\\n')\n",
        "print(timeseries_data.groupby(['ANTENNA'])['TIMESTAMP'].agg(['min', 'max']))\n",
        "print('********************************************************************')\n",
        "\n",
        "print('Average number of daily recordings:\\n')\n",
        "print(\n",
        "    timeseries_data.groupby(['ANTENNA', 'DAY'], as_index=False)['LOAD'].count().groupby(['ANTENNA'])['LOAD'].mean()\n",
        ")\n",
        "print('********************************************************************')\n",
        "\n",
        "print('Count of recorded time points:\\n')\n",
        "print(\n",
        "    timeseries_data.groupby(['ANTENNA', 'HOUR'], as_index=False)['LOAD'].count()\n",
        ")\n",
        "print('********************************************************************')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqr6YlcI0C-J",
        "colab_type": "text"
      },
      "source": [
        "It might be nice to also calculate the minimum, maximum, median and mean network load. Here's an example of calculating the minimum and maximum load per antenna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29j2Vd2b0C-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the minimum and maxmum load per cell\n",
        "timeseries_data.groupby(['ANTENNA'])['LOAD'].agg(['min', 'max'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyiqsNlS0C-M",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Your turn to code!! (I know...scary thought). Your job is to **change the code cell below** such that instead of the `'min'` and `'max'` network load, we calculate...\n",
        "\n",
        "* the `'median'` network load and\n",
        "* the `'mean'` network load per antenna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jcF_wfA0C-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHANGE CODE BELOW to calculate the mean/median network load on each cell\n",
        "timeseries_data.groupby(['ANTENNA'])['LOAD'].agg(['min', 'max'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGOacC3C0C-P",
        "colab_type": "text"
      },
      "source": [
        "**What do all of these statistics tell us about commuter mobile traffic (`antenna 0`) vs. residential mobile traffic (`antenna 1`)?**\n",
        "\n",
        "# Part 3: Visualising time series data\n",
        "\n",
        "---\n",
        "\n",
        "Now we have some basic idea about what makes commuter traffic differ from residential traffic. Let's now **visualise** our time series to really get into the details about how the network load differs for each antenna.\n",
        "\n",
        "There are two main libraries we'll use for data visualisation that we need to import. These are the...\n",
        "\n",
        "* [matplotlib](https://matplotlib.org/) library, which is the main Python plotting library\n",
        "* [seaborn](https://seaborn.pydata.org/index.html), which is a library built on top of matplotlib, specifically for statistics visualisation\n",
        "\n",
        "Let's `import` these libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeFvZnH40C-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# allows the figures to be rendered within the notebook\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIvb1RYf0C-U",
        "colab_type": "text"
      },
      "source": [
        "Let's try and make a graph of our two datasets, using the [sns.lineplot](https://seaborn.pydata.org/generated/seaborn.lineplot.html) function. We'll graph the `TIMESTAMP` on the x-axis, and the `LOAD` on the y-axis. Meaning, we'll see how the network load changes over time. We'll also separate out the commercial traffic from residential."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APVj-_hP0C-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a figure\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Graph the lineplot\n",
        "sns.lineplot(\n",
        "    x='TIMESTAMP',\n",
        "    y='LOAD',\n",
        "    hue='ANTENNA',\n",
        "    data=timeseries_data,\n",
        "    palette=['green', 'red'],\n",
        "    err_style=\"bars\",\n",
        "    ci=\"sd\"\n",
        ")\n",
        "plt.title('Two year network load over entire data collected')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6giE_Bo0C-Y",
        "colab_type": "text"
      },
      "source": [
        "This is **pretty ugly**...and doesn't really give us much information. Let's maybe visualise the data at more **micro-timescales** to see whether we can characterise different traffic types.\n",
        "\n",
        "## Playing with time windows\n",
        "\n",
        "> For our purposes, a **time window** is a section of a time series, that has a specific length. For example, a time window with length 22 time points, starting and including time point 3, would end at time point 24. A time window might also be defined by a length of time. For instance, we might take a time window to be a single day. **Question**: Would we always be guaranteed to have the same number of time points per day?\n",
        "\n",
        "Let's play with our time windows a little bit. What we'll do is graph the following per antenna...\n",
        "\n",
        "* The mean daily time series traffic with its standard deviation (more on this later)\n",
        "* The mean weekly time series traffic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf8tgkLQ0C-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a figure\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Graph the daily traffic\n",
        "fig.add_subplot(2, 1, 1)\n",
        "\n",
        "sns.lineplot(\n",
        "    x='HOUR',\n",
        "    y='LOAD',\n",
        "    hue='ANTENNA',\n",
        "    data=timeseries_data,\n",
        "    palette=['green', 'red'],\n",
        "    err_style=\"bars\",\n",
        "    ci=\"sd\"\n",
        ")\n",
        "plt.title('Daily time series traffic')\n",
        "plt.xticks(range(24))\n",
        "sns.despine()\n",
        "\n",
        "# Graph the weekly traffic\n",
        "fig.add_subplot(2, 1, 2)\n",
        "sns.lineplot(\n",
        "    x='DAYOFWEEK',\n",
        "    y='LOAD',\n",
        "    hue='ANTENNA',\n",
        "    data=timeseries_data,\n",
        "    palette=['green', 'red'],\n",
        "    err_style=\"bars\",\n",
        "    ci=\"sd\"\n",
        ")\n",
        "plt.title('Weekly time series traffic')\n",
        "sns.despine()\n",
        "\n",
        "fig.subplots_adjust(hspace=.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF098ufi0C-a",
        "colab_type": "text"
      },
      "source": [
        "### Thought exercise\n",
        "\n",
        "Work with the person next to you to answer the following questions...\n",
        "\n",
        "* What days/hours has the highest/lowest for each type of antenna? (Commuter/residential)\n",
        "* What numbers indicate days that are weekdays/weekends?\n",
        "* What days/hours is there the **most variation** in network load? (see the following **side-note**)\n",
        "\n",
        "#### Side-note: Standard deviation\n",
        "\n",
        "If you had trouble answering the last question, you can take a detour below (beware...it's a mathy one!). You might have noticed **little bars** on each of our time traffic patterns. Let's zoom in on one...\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Standard_Dev.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "The vertical bar represents **one standard devation** away from the mean of our dataset, specifically both one standard deviation **higher** and **lower** from our mean. Even better...\n",
        "\n",
        "> If our data is **normally distributed**, then we know that 68% of our data falls within one standard deviation from the mean.\n",
        "\n",
        "Ok...back to english. This means that if we take all the data that was **collected at 8AM** for our **antenna 0** data, then 68% of the **network load data points** should fall within the area defined by the bars. Thus, the standard deviation helps us say when hours have **a lot of varied traffic**, or **no variation**. Here's another visual to help...\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Standard_Dev_w_normal.png?raw=1\" width=\"600\">\n",
        "\n",
        "---\n",
        "\n",
        "**Why would this matter if we're trying to build algorithms to differentiate these traffic patterns?**\n",
        "* If there's a lot of variation, would it be easier or harder to do this classification?\n",
        "\n",
        "## Smoothing the curve to look at macro trends\n",
        "\n",
        "Thus far, we have established weekly and daily trends, but there still might be interesting trends we find by looking at the _entire time period_ of data we have. For instance...\n",
        "\n",
        "* Maybe usage is _increasing_ or _decreasing_ throughout the year\n",
        "* Maybe there are specific time periods throughout a year that are unique to each types of traffic\n",
        "\n",
        "Let's re-graph the curve we initially made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjgVFOo-0C-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a figure\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Graph the lineplot\n",
        "sns.lineplot(\n",
        "    x='TIMESTAMP',\n",
        "    y='LOAD',\n",
        "    hue='ANTENNA',\n",
        "    data=timeseries_data,\n",
        "    palette=['green', 'red'],\n",
        "    err_style=\"bars\",\n",
        "    ci=\"sd\"\n",
        ")\n",
        "plt.title('Two year network load over entire data collected')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSynAxtX0C-d",
        "colab_type": "text"
      },
      "source": [
        "It's not super easy to get any relevant info out of this... The issue is, the small micro daily trends inhibit us from seeing the **macro trends** within the data. We call the process of trying to eliminate different trends that are not relevant to our current model view **smoothing**.\n",
        "\n",
        "From [wikipedia](https://en.wikipedia.org/wiki/Smoothing)\n",
        "\n",
        "> To **smooth** a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena\n",
        "\n",
        "Here's a pretty simple example of using smoothing to show macro-trends in temperature over time. **Ignore the red arrow**.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://dawn.cs.stanford.edu/assets/img/2017-08-07-asap/temp.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "As you can see, we are eliminating the details to see the larger picture.\n",
        "\n",
        "### Creating a smoothing function\n",
        "\n",
        "Let's create a **filter**, or a function that **smooths** our curve. This function will smooth a curve using either a...\n",
        "\n",
        "* `monthly`, `daily` or `weekly` average\n",
        "* using either a `median` or `mean` filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDXOgwmn0C-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curve_smoothing(\n",
        "    data, time_interval='daily', filter_type='median', col='LOAD', \n",
        "    title='Network load over entire data collected'\n",
        "):\n",
        "    \"\"\"\n",
        "    Smooth then graph a curve.\n",
        "    \n",
        "    :param data: The data to smooth\n",
        "    :param time_interval: either 'daily', 'weekly' or 'monthly' to smooth\n",
        "    :param filter_type: either 'median' or 'mean', saying to take either the median or the mean filter\n",
        "    :param col: the column we want to graph\n",
        "    :param title: the title ofr the plot\n",
        "    \"\"\"\n",
        "    # Copy the data\n",
        "    data_copy = data.copy()\n",
        "    data_copy['DAY_2'] = [d.day for d in data_copy.TIMESTAMP]\n",
        "    \n",
        "    # Extract daily\n",
        "    if time_interval == 'monthly':\n",
        "        data_copy['FILTER_TIMESTAMP'] = pd.to_datetime(\n",
        "            data_copy['MONTH'].astype(str) + '-' + data_copy['YEAR'].astype(str)\n",
        "        )\n",
        "    # Extract monthly\n",
        "    elif time_interval == 'daily':\n",
        "        data_copy['FILTER_TIMESTAMP'] = pd.to_datetime(\n",
        "            data_copy['MONTH'].astype(str) + '-' + data_copy['DAY_2'].astype(str) + '-'\n",
        "            + data_copy['YEAR'].astype(str)\n",
        "        )\n",
        "    # Extract rolling\n",
        "    elif time_interval == 'weekly':\n",
        "        # Get the 0 and the week numbers\n",
        "        week_start = data_copy.loc[\n",
        "            data_copy['DAYOFWEEK'] == 0, ['TIMESTAMP', 'WEEKNUMBER', 'YEAR']\n",
        "        ].drop_duplicates()\n",
        "        # Rename\n",
        "        week_start.rename(columns={'TIMESTAMP': 'FILTER_TIMESTAMP'}, inplace=True)\n",
        "        # Merge\n",
        "        data_copy = pd.merge(left=data_copy, right=week_start, on=['WEEKNUMBER', 'YEAR'], how='left')        \n",
        "    else:\n",
        "        return None\n",
        "        \n",
        "    # Then groupby type of filter\n",
        "    grouped_data = data_copy.groupby(['ANTENNA', 'FILTER_TIMESTAMP'], as_index=False)[col].agg(filter_type)\n",
        "    \n",
        "    # Graph\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    # Graph the lineplot\n",
        "    sns.lineplot(\n",
        "        x='FILTER_TIMESTAMP',\n",
        "        y=col,\n",
        "        hue='ANTENNA',\n",
        "        data=grouped_data,\n",
        "        palette=['green', 'red'],\n",
        "        err_style=\"bars\",\n",
        "        ci=\"sd\"\n",
        "    )\n",
        "    plt.title('Network load over entire data collected')\n",
        "    sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp5pz5wp0C-f",
        "colab_type": "text"
      },
      "source": [
        "> A **filter**, in this case, aggregates multiple data into a single data point. For example, if we apply a **mean** filter over each **day** in our dataset, we are aggregating all the data points in a single day by taking the average over these data points. We call a filtered curve **smoothed** if we are able to visualise a trend in our dataset post-filtering.\n",
        "\n",
        "### Exercise\n",
        "\n",
        "The following function will **run curve smoothing**. You can change the following parameters...\n",
        "\n",
        "* change `time_interval` from `daily` to `weekly` to `monthly`\n",
        "* change `filter_type` between `mean` and `median`\n",
        "\n",
        "Checkout the resulting graph. What overall trends to do you notice within the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlby6Fpl0C-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHANGE THE PARAMETERS BELOW\n",
        "time_interval = 'monthly'\n",
        "filter_type = 'mean'\n",
        "\n",
        "curve_smoothing(timeseries_data, time_interval=time_interval, filter_type='median')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6iv6UwY0C-h",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: Classifying time series data\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have an idea about what our network data looks like, and what differentiates an antenna that picks up commuter network traffic vs. residential traffic, let's work on a **classification** method to train a computer to recognise what a commuter vs. a residential antenna looks like. Once we train a computer to recognise these traffic patterns, maybe we can use the developed algorithm to report on network traffic changes.\n",
        "\n",
        "So you might be asking, how do we do this? Let's take a little detour into **classification**.\n",
        "\n",
        "### What is classification?\n",
        "\n",
        "> **Classification** is an area of machine learning that tries to build **algorithms** that input a set of data, and output a class label. In our case, we want to build algorithms that **inputs a time series** and output whether the time series follows its normal pattern, or changes outside the norm.\n",
        "\n",
        "If this is too complicated...this [example](https://www.youtube.com/watch?v=vIci3C4JkL0) might help.\n",
        "\n",
        "<img src=\"https://d3ansictanv2wj.cloudfront.net/Figure_1-71076f8ac360d6a065cf19c6923310d2.jpg\" width=\"500\">\n",
        "\n",
        "To do this, we can train what is called a **long short-term memory neural network**.\n",
        "\n",
        "### What is a neural network?\n",
        "\n",
        "\n",
        "A **neural network** is a type of machine learning algorithm that can be used to classify things. It was actually created to resemble how neurons in the brain connect with each other.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1200/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
        "\n",
        "\n",
        "More complex _artificial_ neural network models look like the gif below.\n",
        "\n",
        "![](https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-max-1mb.gif)\n",
        "\n",
        "In the image, we're feeding the **image with the number 7** to the **input layer** of the network. The image has a certain amount of pixels which is fed into an **input** layer. The network has been trained to recognise the seven, and you can see there are certain nodes/edges activated in the **middle/hidden layers**, and **output layers** that are specifically activated when a seven is inputted in the network.\n",
        "\n",
        "Since this is a network with input, hidden and output layers, we call this type of neural network a **deep neural network**, and this type of machine learning **deep learning**.\n",
        "\n",
        "#### What does this have to do with our time series?\n",
        "\n",
        "Instead of training a network to recognise an image, maybe we can **train a neural network** to recognise a **patterns** in our data that constitute **normal commuter or residential traffic** on an attenna. Once it realises these normal patterns, maybe we can train the same network to recognise an environmental change that might affect the network.\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/ANN.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "## Training a basic network\n",
        "\n",
        "Let's train a basic neural network to recognise our two traffic patterns. What we'll do is the following...\n",
        "\n",
        "* We'll divide our dataset into two datasets. The first dataset will include the first year of our time series data, and the second dataset will use the second year of data\n",
        "* We'll then _train_ our network on the first year of data, and then use the second year of data to _assess_ model performance\n",
        "\n",
        "We also need to define a _time window_ in a variable called `window_width`...meaning, how many time points will we feed to our network? This translates into the _size of the input layer_ of the network. Really, we're trying to tell the network how much data is needed to distinguish the type of traffic.\n",
        "\n",
        "Run the code below to create a function that will make a dataset with a given time window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUWgzH70C-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shift_data(data, window_width, col):\n",
        "    \"\"\"\n",
        "    Shift data with a given window width\n",
        "    \n",
        "    :param data: the dataset\n",
        "    :param window_width: the window widths to shift by\n",
        "    :param col: the column to shift\n",
        "    \n",
        "    :return data_copy: the data with shifted columns\n",
        "    :return feature_cols: the names of the created columns\n",
        "    \"\"\"\n",
        "    # Copy data\n",
        "    data_copy = data.copy()\n",
        "    data_copy.sort_values(['ANTENNA', 'TIMESTAMP'], inplace=True)\n",
        "    data_copy.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Shift\n",
        "    feature_cols = ['LOAD']\n",
        "    all_df = []\n",
        "    for a in data_copy.ANTENNA.unique():\n",
        "        temp = data_copy.loc[data_copy['ANTENNA'] == a, :]\n",
        "        for i in range(1, window_width):\n",
        "            shifted_data = temp[col].shift(i)\n",
        "            temp['LOAD_' + str(i)] = None\n",
        "            if 'LOAD_' + str(i) not in feature_cols:\n",
        "                feature_cols.append('LOAD_' + str(i))\n",
        "            temp.loc[shifted_data.index, 'LOAD_' + str(i)] = shifted_data\n",
        "        all_df.append(temp)\n",
        "                \n",
        "    return pd.concat(all_df).reset_index(drop=True).dropna(), feature_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPyCDZwh0C-k",
        "colab_type": "text"
      },
      "source": [
        "Since antenna `0` and `1` had an average of 22 and 11 timepoints per day respectively, let's choose the larger of the two numbers as our time window (so we always capture at least one day of data per antenna). We'll come back to the idea of a time window later. Run the code below to prep the data. We'll also create the `train` and `test` datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgR5bfpI0C-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create shifts\n",
        "timeseries_w_shift, feature_cols = shift_data(timeseries_data, window_width=22, col='LOAD')\n",
        "\n",
        "# Now create dummy variables\n",
        "timeseries_w_shift = pd.concat([timeseries_w_shift, pd.get_dummies(timeseries_w_shift[['SERVICE']])], axis=1)\n",
        "\n",
        "# Split data in half\"\n",
        "train = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] < '2017-06-30', :]\n",
        "test = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] >= '2017-06-30', :]\n",
        "\n",
        "# Show result\n",
        "timeseries_w_shift[feature_cols].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGV7wyY0C-o",
        "colab_type": "text"
      },
      "source": [
        "#### Side-note: train vs. test sets\n",
        "\n",
        "Why did we split our data into two datasets, a `train` and a `test` set? Why do we not train and test model performance on the same dataset? Think about studying for a math test...if we practiced using just the problems we already have completed, we'd get really good at understanding those problems, but not necessarily be able to understand new information.\n",
        "\n",
        "In machine learning, we do not want computers to just understand the data we have at hand, we want to see how it will predict new data it has not been exposed to you. Thus, test sets are held out of model training, and are used to simulate what it's like to expose our algorithms to new information.\n",
        "\n",
        "#### Another side-note...dummy variables\n",
        "\n",
        "You may have noticed we used the [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function in the code above. Dummy variables are binary variables created out of categorical variables. What we did was take the first column on the left-hand side, and turn it into the two columns on the right-hand side...\n",
        "\n",
        "| SERVICE | --------------- | SERVICE_commuter | SERVICE_residential |\n",
        "| - | - | - | - |\n",
        "| commuter | - | 1 | 0 | \n",
        "| residential | - | 0 | 1 |\n",
        "| residential | - | 0 | 1 |\n",
        "| commuter | - | 1 | 0 |\n",
        "| ... | - | ... | ... | \n",
        "\n",
        "This is because computers do not like **words (or strings)**, but do like numbers. Also, creating **two columns** will allow us to train a neural network that can recognise whether a traffic pattern is more like a commuter _or_ residential load.\n",
        "\n",
        "#### One last side-note...promise!!\n",
        "\n",
        "We actually trained a network with **two different nodes** in the output layer. One node gives the _probability_ that a signal is carrying commuter traffic, and the other node gives the proability that a signal is carrying residential traffic.\n",
        "\n",
        "We could have trained just one node that distinguishes either commuter or residential traffic. High probabilities would designate one type of traffic, and low probabilities would designate another type of traffic. \n",
        "\n",
        "So...why do you think we made two output nodes?\n",
        "\n",
        "### Network training\n",
        "\n",
        "Let's actually train the network...the code below will train a network based upon a given window_width. To train the network, we'll use the [Keras](https://keras.io/) library. Keras is a library, often powered by [tensorflow](https://www.tensorflow.org/) (Google's main neural network engine), that can be used to create and train neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "x9CNctjP0C-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create function\n",
        "def create_ann(data, window_width):\n",
        "    \"\"\"\n",
        "    Build a neural network using a given dataset and window_width\n",
        "    \n",
        "    :param data: the dataset\n",
        "    :param window_width: the window width\n",
        "    \n",
        "    :return model: return a fully trained networked\n",
        "    \"\"\"\n",
        "    # Build the network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=window_width, activation='relu'))\n",
        "    model.add(Dense(units=10, activation='relu'))\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the network\n",
        "    model.fit(\n",
        "        x=data[feature_cols].values, \n",
        "        y=data[['SERVICE_commuter', 'SERVICE_residential']].values,\n",
        "        epochs=20,\n",
        "        batch_size=round(timeseries_w_shift.shape[0] / 10),\n",
        "        validation_split=0.05, \n",
        "        verbose=0,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_ann(train, window_width=22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6jqQ5WN0C-t",
        "colab_type": "text"
      },
      "source": [
        "Let's see how well our network performs. We can do this by predicting the _type_ of network using _just the second year of data_ that we did not use for model training. Ideally, all of the `antenna 0` data would be marked as a `commuter`, and all of the `antenna 1 data` would be marked as `residential`. \n",
        "\n",
        "We'll graph our results as well, and use the [sklearn roc_auc score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). Ideally, this score should be a \"1\" if our model is doing well. If the score is 0.5, the model is not really able to differentiate a commuter/residential signal. You can read more about ROC AUC scores [here](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oaj3BqHu0C-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import roc AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predict results\n",
        "y_pred_comm = pd.DataFrame(\n",
        "    model.predict(test[feature_cols].values), columns=['Commuter Prediction', 'Residential Prediction']\n",
        ")\n",
        "y_pred_comm.index = test.index\n",
        "\n",
        "# Get AUC score\n",
        "auc = roc_auc_score(test[['SERVICE_commuter', 'SERVICE_residential']].values, y_pred_comm.values)\n",
        "print('AUC Score: %.2f' % auc)\n",
        "\n",
        "# Add to the DataFrame\n",
        "test_copy = test.copy()\n",
        "test_copy = pd.concat([test_copy, y_pred_comm], axis=1, sort=False)\n",
        "\n",
        "curve_smoothing(\n",
        "    test_copy, time_interval='weekly', filter_type='mean', col='Commuter Prediction', \n",
        "    title='Commuter prediction on test data'\n",
        ")\n",
        "curve_smoothing(\n",
        "    test_copy, time_interval='weekly', filter_type='mean', col='Residential Prediction',\n",
        "    title='Residential prediction on test data'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he4SiyYg0C-w",
        "colab_type": "text"
      },
      "source": [
        "This model is...not doing too well. Ideally, we'd want all the `ANTENNA = 1` lines to be at `1` for the commuter prediction, and `0` for the residential prediction. We'd want the opposite for the `ANTENNA = 0` lines.\n",
        "\n",
        "Let's dive into the concept of **window widths** a little bit more, which might solve this problem.\n",
        "\n",
        "### Window widths\n",
        "\n",
        "Currently, we've given our model our model a `window_width = 22`, meaning, the model is given multiple chunks **22 time points** to learn about whether a given time series is a commuter or residential time series. Is this the correct amount of data we want to give our model?\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Window_Widths.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "As you can see, a **larger width** will give more of our data at once to a model to train.\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Maybe we're not giving enough data to the input layer of our model.\n",
        "\n",
        "Adjust the window widths using the `window_width` parameter, and run the below code to retrain and plot a model. Does our model training improve?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ZEaxizxH0C-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHANGE WINDOW WIDTH HERE\n",
        "window_width = 100\n",
        "\n",
        "# Create shifts\n",
        "timeseries_w_shift, feature_cols = shift_data(timeseries_data, window_width=window_width, col='LOAD')\n",
        "\n",
        "# Now create dummy variables\n",
        "timeseries_w_shift = pd.concat([timeseries_w_shift, pd.get_dummies(timeseries_w_shift[['SERVICE']])], axis=1)\n",
        "\n",
        "# Split data in half\n",
        "train = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] < '2017-06-30', :]\n",
        "test = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] >= '2017-06-30', :]\n",
        "\n",
        "# Train model\n",
        "model = create_ann(train, window_width=window_width)\n",
        "\n",
        "# Predict results\n",
        "y_pred_comm = pd.DataFrame(\n",
        "    model.predict(test[feature_cols].values), columns=['Commuter Prediction', 'Residential Prediction']\n",
        ")\n",
        "y_pred_comm.index = test.index\n",
        "\n",
        "# Get AUC score\n",
        "auc = roc_auc_score(test[['SERVICE_commuter', 'SERVICE_residential']].values, y_pred_comm.values)\n",
        "print('AUC Score: %.2f' % auc)\n",
        "\n",
        "# Add to the DataFrame\n",
        "test_copy = test.copy()\n",
        "test_copy = pd.concat([test_copy, y_pred_comm], axis=1, sort=False)\n",
        "\n",
        "curve_smoothing(\n",
        "    test_copy, time_interval='weekly', filter_type='mean', col='Commuter Prediction', \n",
        "    title='Commuter prediction on test data'\n",
        ")\n",
        "curve_smoothing(\n",
        "    test_copy, time_interval='weekly', filter_type='mean', col='Residential Prediction',\n",
        "    title='Residential prediction on test data'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U17VgOL40C-y",
        "colab_type": "text"
      },
      "source": [
        "## Building memory into a network\n",
        "\n",
        "The main issue with using neural networks for this problem is that every **time window is treated as a new example** in the network. There's absolutely no linkage in our current network for signals that follow each other within the same antenna if they do not exist within the same time window. There is a way to train a computer to connect the dots between two different, but following time windows, and allow a computer to **remember features** that make a time series distinct. We'll need to teach a computer to **remember** linkages between different time windows.\n",
        "\n",
        "### What is memory to a computer?\n",
        "\n",
        "Let's think about how you read a sentence. Pretend I have the following sentence, and it's your job to fill in the blank...\n",
        "\n",
        "> I promised to feed Linda's cat for the week. I had to run across town yesterday to buy food for **____** so I could go over to Linda's house this afternoon to feed it.\n",
        "\n",
        "Let's think about how your brain fills in the blank. There's a few key words, prior to the blank, that allows you to infer the missing word, highlighted below.\n",
        "\n",
        "> I promised to **feed** Linda's **cat** for the week. I had to run across town yesterday to **buy food** for **____** so I could go over to Linda's house this afternoon to feed it.\n",
        "\n",
        "Now pretend we had a neural network that was trying to predict the blank word, and it was only given the fixed amount of words prior to the blank. Here's the words that the network would be given based upon given window widths.\n",
        "\n",
        "| Window width | Word features |\n",
        "| ------------ | ------------- |\n",
        "| 3 | \"buy\", \"food\", \"for\" |\n",
        "| 10 | \"had\", \"to\", \"run\", \"across\", \"town\", \"yesterday\", \"to\", \"buy\", \"food\", \"for\" |\n",
        "| 17 | \"feed\", \"Linda's\", \"cat\", \"for\", \"the\", \"week\", \"I\", \"had\", \"to\", \"run\", \"across\", \"town\", \"yesterday\", \"to\", \"buy\", \"food\", \"for\" |\n",
        "\n",
        "So...you might conclude that we should have about 17 words as our window width. The issue is, like we **trained** on one dataset and **tested** on another dataset above, we are not going to deploy the algorithm to always fill in the blank for the same sentence (that would be useless...). We might want our algorithm to fill in the blank within the following setences instead.\n",
        "\n",
        "* **Sentence 1:** Dave has a cat named Alfred. He often goes to the pet-store once a month to buy food for his ___.\n",
        "* **Sentence 2:** My sister often watches the neighbours' two children and their cat, usually about once a month. The last time she babysat, she was extremely grumpy, as there was no food for the cat, and thus she had to run home and borrow mum's car to buy ___ food.\n",
        "\n",
        "The window width for sentence 1 would be about 17, but the width for sentence 2 would be _much_ longer. Imagine if we had an entire story that our computer was ingesting information from, and the main character's cat was mentioned early in the story, but not brought up again until 100 pages later!!\n",
        "\n",
        "**Side note:** We're treating sentences like time series in the examples above. This is actually what's done in the field of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing). There a really [cool article](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html) from the NY Times that talks about how modeling a sentence as a time series, and then using a neural network, changed the face of Google's Translate algorithm.\n",
        "\n",
        "So...a logical answer to this problem would be to make the time window in our network really big, so that we cover all these potential cases. Let's see what happens when we fit our network with a really big time window.. We'll time different training times with varying window widths using the [time](https://docs.python.org/3/library/time.html) library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G48RkMSo0C-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "# Window width of 22\n",
        "start = time.time()\n",
        "window_width = 22\n",
        "timeseries_w_shift, feature_cols = shift_data(timeseries_data, window_width=window_width, col='LOAD')\n",
        "timeseries_w_shift = pd.concat([timeseries_w_shift, pd.get_dummies(timeseries_w_shift[['SERVICE']])], axis=1)\n",
        "train = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] < '2017-06-30', :]\n",
        "test = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] >= '2017-06-30', :]\n",
        "model = create_ann(train, window_width=window_width)\n",
        "y_pred_comm = pd.DataFrame(\n",
        "    model.predict(test[feature_cols].values), columns=['Commuter Prediction', 'Residential Prediction']\n",
        ")\n",
        "y_pred_comm.index = test.index\n",
        "auc = roc_auc_score(test[['SERVICE_commuter', 'SERVICE_residential']].values, y_pred_comm.values)\n",
        "end = time.time()\n",
        "print('AUC Score: %.2f' % auc)\n",
        "print('Total training time was %.2f seconds\\n' % (end - start))\n",
        "\n",
        "# Window width of 100\n",
        "start = time.time()\n",
        "window_width = 100\n",
        "timeseries_w_shift, feature_cols = shift_data(timeseries_data, window_width=window_width, col='LOAD')\n",
        "timeseries_w_shift = pd.concat([timeseries_w_shift, pd.get_dummies(timeseries_w_shift[['SERVICE']])], axis=1)\n",
        "train = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] < '2017-06-30', :]\n",
        "test = timeseries_w_shift.loc[timeseries_w_shift['TIMESTAMP'] >= '2017-06-30', :]\n",
        "model = create_ann(train, window_width=window_width)\n",
        "y_pred_comm = pd.DataFrame(\n",
        "    model.predict(test[feature_cols].values), columns=['Commuter Prediction', 'Residential Prediction']\n",
        ")\n",
        "y_pred_comm.index = test.index\n",
        "auc = roc_auc_score(test[['SERVICE_commuter', 'SERVICE_residential']].values, y_pred_comm.values)\n",
        "end = time.time()\n",
        "print('AUC Score: %.2f' % auc)\n",
        "print('Total training time was %.2f seconds\\n' % (end - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYVYvPGH0C-2",
        "colab_type": "text"
      },
      "source": [
        "So, tldr; the window width _increases_ the training time dramatically, and _does not help the accuracy of the model!_ So...is there a way to...\n",
        "\n",
        "* Have a computer somehow link smaller/bigger window widths together?\n",
        "* Even better, is it possible for a computer to somehow **dynamically piece together the relevant information of various time windows??**\n",
        "* ...and also a shorter runtime would be lovely\n",
        "\n",
        "## Long short-term memory (LSTM) networks\n",
        "\n",
        "There is a way to do this, using the concept of a **long short-term memory, or LSTM** network. The LSTM network is a type of **Recurrent Neural Network (RNN)**. RNN's are a special network architecture that adds information about a previous training example output into the next training example.\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
        "\n",
        "Imagine the box on the left is a first time point with a specific window width, the center box is the next time point with the consecutive window, and so-on and so-forth. What we can do is train the network on one time point, and then _pass_ its information into training the network on the following time point.\n",
        "\n",
        "The main issue with basic RNN's is that they do well at passing on information between time points within close proximity to each other, but not time points that are **far apart**. LSTM's fix this by carrying forward more information about the previous network, and then selectively deciding what to keep/forget gradually. Here's a picture of the inside of an LSTM.\n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "It's pretty complex, and I'm not going to go too into detail on the innerworkings of an LSTM, but [this post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) does a great job of teaching LSTM's and how they can be advantageous. The main points are...\n",
        "\n",
        "* LSTM's have information they pass between each \"cell\" box **A** in the network. This information is not just a previous output, but a \"state\" that gets updated with each subsequent example in time\n",
        "* Within each subsequent cell, the LSTM's look at the current state of the cell and the new information, and then decide what to selectively change about the cell state\n",
        "* LSTM's then output an answer for the current cell ($h_t$), and then pass this output and the updated cell state onto the next cell\n",
        "\n",
        "### Building an LSTM\n",
        "\n",
        "Let's actually build an LSTM. We'll need to import the right keras module, but then we can build a function to make the network.\n",
        "\n",
        "#### Side-node: dropout\n",
        "\n",
        "You will see in the LSTM building below that we add something called **Dropout**. Dropout helps us control overfitting. Overfit models work _really well_ on the data we used to train on model, but not other datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi7IOjkl0C-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM, Dropout\n",
        "\n",
        "def create_lstm(data, data_labels, window_width, num_features):\n",
        "    \"\"\"\n",
        "    Build an LSTM using a given sequence length and feature length\n",
        "    \n",
        "    :param data: the data for the model\n",
        "    :param data_labels: the data labels for the model\n",
        "    :param window_width: the amount of data to feed to each cell\n",
        "    :param num_features: the number of features inputted per time point\n",
        "    \n",
        "    :return model: the model\n",
        "    \"\"\"\n",
        "    # Build the network\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(input_shape=(window_width, num_features), units=100, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Fit\n",
        "    model.fit(\n",
        "        x=data,\n",
        "        y=data_labels,\n",
        "        epochs=10,\n",
        "        batch_size=round(data.shape[0] / 10), \n",
        "        validation_split=0.05, \n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U04P3SE0C-5",
        "colab_type": "text"
      },
      "source": [
        "We'll need to do some data prep for our LSTM to work. The LSTM takes in an array that is the following size...\n",
        "\n",
        "`(num_data_samples, window_width, features_per_width)`\n",
        "\n",
        "This is actually a **3D array**. Thank you to [this azure code on GitHub](https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb) for the help with building these input arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srlfwepr0C-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_sequence(data, window_width, features):\n",
        "    \"\"\"\n",
        "    Generate an input array based upon a dataframe, window_width, and columns\n",
        "    \n",
        "    :param data: the data\n",
        "    :param window_width: the window width to use\n",
        "    :param features: the features to use\n",
        "    \n",
        "    :return data_array[start:stop, :]: the array with the correct features in the right shape\n",
        "    \"\"\"\n",
        "    data_array = data[features].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements - window_width), range(window_width, num_elements)):\n",
        "        yield data_array[start:stop, :]\n",
        "        \n",
        "def gen_labels(data, window_width, label):\n",
        "    \"\"\"\n",
        "    Generate labels for the input\n",
        "    \n",
        "    :param data: the data\n",
        "    :param window_width: the window width to use\n",
        "    :param label: the label column for output\n",
        "    \n",
        "    :return data_array[seq_length:num_elements]: the correct section of the data array\n",
        "    \"\"\"\n",
        "    data_array = data[label].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    return data_array[window_width:num_elements]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8RI2bE80C-7",
        "colab_type": "text"
      },
      "source": [
        "Let's now run the code below that will let us format inputs to the LSTM. We will use the [numpy](https://www.numpy.org/) library as well, which has many mathematical functions useful for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7xw4rAo0C-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import numpy\n",
        "import numpy as np\n",
        "\n",
        "def generate_train_test_sets(data, window_width, features, timestamp_cutoff='2017-06-30'):\n",
        "    \"\"\"\n",
        "    Prep data for the LSTM\n",
        "    \n",
        "    :param data: the timeseries data\n",
        "    :param window_width: the window width to use\n",
        "    :param features: the features to use\n",
        "    :param timestamp_cutoff: when to cut our features\n",
        "    \n",
        "    :return train_array: the training data\n",
        "    :return train_label_array: the training labels\n",
        "    :return test_array: the testing labels\n",
        "    :return test: the test dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort data\n",
        "    data.sort_values(['ANTENNA', 'TIMESTAMP'], inplace=True)\n",
        "\n",
        "    # Split\n",
        "    train = data.loc[data['TIMESTAMP'] < timestamp_cutoff, :]\n",
        "    test = data.loc[data['TIMESTAMP'] >= timestamp_cutoff, :]\n",
        "    \n",
        "    # Set test\n",
        "    if test.shape[0] == 0:\n",
        "        test = train.copy()\n",
        "\n",
        "    # Make data\n",
        "    train_gen = (list(gen_sequence(train.loc[train['ANTENNA'] == a, :], window_width, features)) for a in [0, 1])\n",
        "    train_array = np.concatenate(list(train_gen)).astype(np.float32)\n",
        "    test_gen = (list(gen_sequence(test.loc[test['ANTENNA'] == a, :], window_width, features)) for a in [0, 1])\n",
        "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
        "\n",
        "    # Generate labels\n",
        "    train_label_gen = [gen_labels(train.loc[train['ANTENNA'] == a, :], window_width, 'SERVICE') for a in [0, 1]]\n",
        "    train_label_array = pd.get_dummies(np.concatenate(train_label_gen)).values\n",
        "    \n",
        "    return train_array, train_label_array, test_array, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkzE3aP40C-9",
        "colab_type": "text"
      },
      "source": [
        "Now we can train our LSTM on the first year of data (as before) and then predict the output on the final year. Let's make a function to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wnLosZB0C--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lstm(data, window_width, features, timestamp_cutoff='2017-06-30'):\n",
        "    \"\"\"\n",
        "    Run LSTM\n",
        "    \n",
        "    :param data: the timeseries data\n",
        "    :param window_width: the window width to use\n",
        "    :param features: the features to use\n",
        "    :param timestamp_cutoff: when to cut our features\n",
        "    \n",
        "    :return model: the model used\n",
        "    \"\"\"\n",
        "    # Generate data\n",
        "    train_array, train_label_array, test_array, test = generate_train_test_sets(\n",
        "        data, window_width, features, timestamp_cutoff\n",
        "    )\n",
        "    \n",
        "    # Build model\n",
        "    model = create_lstm(\n",
        "        train_array, train_label_array, \n",
        "        window_width=test_array.shape[1], num_features=test_array.shape[2]\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    y_pred = pd.DataFrame(\n",
        "        model.predict(test_array), columns=['Commuter Prediction', 'Residential Prediction']\n",
        "    )\n",
        "\n",
        "    # Add to the DataFrame\n",
        "    test_copy = test.copy()\n",
        "    test_copy['Commuter Prediction'] = None\n",
        "    test_copy['Residential Prediction'] = None\n",
        "    curr = 0\n",
        "    indices = []\n",
        "    for a in test_copy.ANTENNA.unique():\n",
        "        temp = test_copy.loc[test_copy['ANTENNA'] == a, :].index.tolist()\n",
        "        indices += temp[window_width:len(temp)]\n",
        "    y_pred.index = indices\n",
        "    test_copy.loc[y_pred.index, 'Commuter Prediction'] = y_pred['Commuter Prediction']\n",
        "    test_copy.loc[y_pred.index, 'Residential Prediction'] = y_pred['Residential Prediction']\n",
        "    test_copy['Commuter Prediction'] = test_copy['Commuter Prediction'].astype(float)\n",
        "    test_copy['Residential Prediction'] = test_copy['Residential Prediction'].astype(float)\n",
        "    test_copy.dropna(inplace=True)\n",
        "    test_label_array = pd.get_dummies(test_copy['SERVICE'])\n",
        "    \n",
        "    # Get AUC score\n",
        "    auc = roc_auc_score(\n",
        "        test_label_array[['commuter', 'residential']].values, \n",
        "        test_copy[['Commuter Prediction', 'Residential Prediction']].values\n",
        "    )\n",
        "    print('\\nAUC Score: %.2f\\n' % auc)\n",
        "\n",
        "    # Plot\n",
        "    curve_smoothing(\n",
        "        test_copy, time_interval='weekly', filter_type='mean', col='Commuter Prediction', \n",
        "        title='Commuter prediction on test data'\n",
        "    )\n",
        "    curve_smoothing(\n",
        "        test_copy, time_interval='weekly', filter_type='mean', col='Residential Prediction',\n",
        "        title='Residential prediction on test data'\n",
        "    )\n",
        "\n",
        "    # Return the model\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahQLZKY40C-_",
        "colab_type": "text"
      },
      "source": [
        "Now let's run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8wLcr5h0C_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "window_width = 22\n",
        "features = ['LOAD']\n",
        "\n",
        "# Run\n",
        "run_lstm(timeseries_data, window_width, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rMyznJFvYVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = run_lstm(timeseries_data, window_width, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bot5Y0830C_B",
        "colab_type": "text"
      },
      "source": [
        "A lot better, as we're definitely starting to get the separation we want to between each classifier. Let's play around with a couple of things.\n",
        "\n",
        "### Exercise\n",
        "\n",
        "In the cell below, play around with the `window_width` and try to choose an optimal width for training your model. Use the graphs to decide if a specific window width is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Z9jcG-0C_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "window_width = 100\n",
        "features = ['LOAD']\n",
        "\n",
        "# Run\n",
        "run_lstm(timeseries_data, window_width, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2dN6svO0C_D",
        "colab_type": "text"
      },
      "source": [
        "Let's run a final model. This time, we'll use _all of our data_ to train the model. We'll even include some extra features about the date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztqotpeB0C_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "window_width = 22\n",
        "features = ['LOAD', 'MONTH', 'HOUR', 'DAY', 'DAYOFWEEK', 'WEEKNUMBER', 'YEAR']\n",
        "timestamp_cutoff = '2019-01-01'\n",
        "\n",
        "# Run\n",
        "end_model = run_lstm(timeseries_data, window_width, features, timestamp_cutoff)\n",
        "\n",
        "# Final window width\n",
        "final_window_width = window_width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9fxXAZZ0C_G",
        "colab_type": "text"
      },
      "source": [
        "# Part 5: Detecting changes to the network environment\n",
        "\n",
        "---\n",
        "\n",
        "Let's review what we've learned thus far...\n",
        "\n",
        "1. We learned about time series, and different methods of visualising our specific dataset within specific time ranges. We also learned how we can apply different smoothing techniques which remove irrelevant microtrends within our dataset.\n",
        "2. We developed a simple method that tried classify normal trends within a dataset, but it did not capture enought information about the relationships between subsequent parts of the same time series\n",
        "3. We then created a more advanced classifier using the concept of **memory** in a neural network, that was able to recognise and characterise our two types of mobile network traffic data\n",
        "\n",
        "And now for the true power of what we've done...**can we detect changes to network usage when the environment around a cell tower changes?**\n",
        "\n",
        "Let's return back to environmental changes described in the picture below...\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/CoderAcademyEdu/data_science_sc_student/blob/master/img/Problem_Scenario_w_new_apt.png?raw=1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "What do you think happens to our commuter traffic pattern when an apartment complex begins to be built between the antenna and the train station the antenna serves? Let's find out, and see if we can catch when the captured network load changes.\n",
        "\n",
        "First we'll need to import some data, and prep it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvketOWD0C_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload new data\n",
        "new_data = pd.read_csv('https://raw.githubusercontent.com/CoderAcademyEdu/data_science_sc_student/master/data/traffic_workshop_testing_data.csv')\n",
        "new_data['TIMESTAMP'] = pd.to_datetime(new_data['TIMESTAMP'])\n",
        "new_data['MONTH'] = [d.month for d in new_data['TIMESTAMP']]\n",
        "new_data['HOUR'] = [d.hour for d in new_data['TIMESTAMP']]\n",
        "new_data['DAY'] = [d.dayofyear for d in new_data['TIMESTAMP']]\n",
        "new_data['DAYOFWEEK'] = [d.dayofweek for d in new_data['TIMESTAMP']]\n",
        "new_data['WEEKNUMBER'] = [d.isocalendar()[1] for d in new_data['TIMESTAMP']]\n",
        "new_data['YEAR'] = [d.year for d in new_data['TIMESTAMP']]\n",
        "\n",
        "# Capture just the antenna\n",
        "antenna_0 = new_data.loc[new_data['ANTENNA'] == 0, :].reset_index(drop=True).sort_values(by=['TIMESTAMP'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhbHQ9Hd0C_I",
        "colab_type": "text"
      },
      "source": [
        "The following code below will create a simulation of observing the traffic throughout a year as the apartment complex is built up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SwUCCLK00C_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Plot data as traffic progresses throughout time\n",
        "plot_width = 500\n",
        "\n",
        "# First set up the figure, the axis, and the plot element we want to animate\n",
        "fig, ax = plt.subplots(figsize=(15, 7))\n",
        "ax.set_xlim((antenna_0.TIMESTAMP.min(), antenna_0.TIMESTAMP.max()))\n",
        "ax.set_ylim((0, 15000))\n",
        "ax.set_xlabel('TIMESTAMP')\n",
        "ax.set_ylabel('LOAD')\n",
        "ax.set_title('Network load over time')\n",
        "line, = ax.plot([], [], lw=2)\n",
        "\n",
        "# animation function. This is called sequentially\n",
        "def animate(i):\n",
        "    \"\"\"\n",
        "    Animate the network traffic simulation\n",
        "    \n",
        "    :param i: the chunk for the current plotting window\n",
        "    \"\"\"\n",
        "    end = min(i * 100 + plot_width, antenna_0.shape[0] - 1)\n",
        "    data_copy = antenna_0.loc[0:end, :].copy()\n",
        "    data_copy['DAY_2'] = [d.day for d in data_copy.TIMESTAMP]\n",
        "    data_copy['FILTER_TIMESTAMP'] = pd.to_datetime(\n",
        "            data_copy['MONTH'].astype(str) + '-' + data_copy['DAY_2'].astype(str) + '-'\n",
        "            + data_copy['YEAR'].astype(str)\n",
        "    )\n",
        "    grouped_data = data_copy.groupby(['FILTER_TIMESTAMP'], as_index=False)['LOAD'].agg('mean')\n",
        "    line.set_data(grouped_data['FILTER_TIMESTAMP'], grouped_data['LOAD'])\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "# call the animator. blit=True means only re-draw the parts that have changed.\n",
        "anim = FuncAnimation(\n",
        "    fig, animate,\n",
        "    frames=round((antenna_0.shape[0] - plot_width) / 100), blit=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXQH_Gy90C_K",
        "colab_type": "text"
      },
      "source": [
        "### Thought exercise\n",
        "\n",
        "Run the simulation below. Visually, at what time do you believe the apartment complex impacts the antenna?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G07b2NHM0C_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwB5TXHD0C_M",
        "colab_type": "text"
      },
      "source": [
        "## Adding the prediction\n",
        "\n",
        "Let's use our model to analyse the prediction. We'll need to build in a real-time prediction using the `end_model` we already built. The following function will build a data prep and prediction layer into our visualisation code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bATEtqeH0C_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Plot data as traffic changes throughout time\n",
        "plot_width = 500\n",
        "\n",
        "# First set up the figure, the axis, and the plot element we want to animate\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
        "ax1.set_xlim((antenna_0.TIMESTAMP.min(), antenna_0.TIMESTAMP.max()))\n",
        "ax1.set_ylim((0, 15000))\n",
        "ax1.set_ylabel('LOAD')\n",
        "ax1.set_title('Simulation of network monitoring')\n",
        "ax2.set_ylabel('Likelihood of normal traffic')\n",
        "ax2.set_xlabel('TIMESTAMP')\n",
        "\n",
        "ax2.set_xlim((antenna_0.TIMESTAMP.min(), antenna_0.TIMESTAMP.max()))\n",
        "ax2.set_ylim((-0.05, 1.05))\n",
        "\n",
        "line1, = ax1.plot([], [], lw=2)\n",
        "line2, = ax2.plot([], [], lw=2, color='r')\n",
        "\n",
        "def animate_w_prediction(i):\n",
        "    \"\"\"\n",
        "    Animate with predictions\n",
        "    \n",
        "    :param i: the chunk for the current plotting window\n",
        "    \"\"\"\n",
        "    end = min(i * 100 + plot_width, antenna_0.shape[0] - 1)\n",
        "    data_copy = antenna_0.loc[0:end, :].copy()\n",
        "    \n",
        "    # Prep data\n",
        "    antenna_0_gen = (list(\n",
        "        gen_sequence(\n",
        "            data_copy.loc[data_copy['ANTENNA'] == a, :], final_window_width, features)\n",
        "    ) for a in [0])\n",
        "    antenna_0_array = np.concatenate(list(antenna_0_gen)).astype(np.float32)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = pd.DataFrame(\n",
        "        end_model.predict(antenna_0_array), columns=['Commuter Prediction', 'Residential Prediction']\n",
        "    )\n",
        "    y_pred['TIMESTAMP'] = data_copy.loc[final_window_width:end, 'TIMESTAMP'].copy()\n",
        "    \n",
        "    # Group data\n",
        "    data_copy['DAY_2'] = [d.day for d in data_copy.TIMESTAMP]\n",
        "    data_copy['FILTER_TIMESTAMP'] = pd.to_datetime(\n",
        "            data_copy['MONTH'].astype(str) + '-' + data_copy['DAY_2'].astype(str) + '-'\n",
        "            + data_copy['YEAR'].astype(str)\n",
        "    )\n",
        "    grouped_data = data_copy.groupby(['FILTER_TIMESTAMP'], as_index=False)['LOAD'].agg('mean')\n",
        "    \n",
        "    # Plot\n",
        "    line1.set_data(grouped_data['FILTER_TIMESTAMP'], grouped_data['LOAD'])\n",
        "    line2.set_data(y_pred['TIMESTAMP'], y_pred['Commuter Prediction'])\n",
        "    \n",
        "    return (line1, line2)\n",
        "\n",
        "# call the animator. blit=True means only re-draw the parts that have changed.\n",
        "anim_w_pred = FuncAnimation(\n",
        "    fig, animate_w_prediction,\n",
        "    frames=round((antenna_0.shape[0] - plot_width) / 100), blit=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEE0Myyn0C_O",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's build our animation, and see how our algorithm would perform in-practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "W3M_gg5V0C_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HTML(anim_w_pred.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA0hQQ2V0C_Q",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Let's put everything together! The following code has three steps that can be run subsequently to...\n",
        "\n",
        "1. Train a model given a specific window width\n",
        "2. Create the animation\n",
        "3. Run the simulation\n",
        "\n",
        "Play with adjusting the window width and seeing how it impacts the final simulation. **YOU WILL NEED TO RUN ALL THREE CELLS TO SEE THE ENTIRE EXERCISE RUN END-TO-END!**\n",
        "\n",
        "**1. Run this cell** to train a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-fI0XqvS0C_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameters\n",
        "window_width = 100\n",
        "features = ['LOAD', 'MONTH', 'HOUR', 'DAY', 'DAYOFWEEK', 'WEEKNUMBER', 'YEAR']\n",
        "timestamp_cutoff = '2019-01-01'\n",
        "\n",
        "# Run\n",
        "print('Training model...')\n",
        "end_model = run_lstm(timeseries_data, window_width, features, timestamp_cutoff)\n",
        "print('Model trained')\n",
        "\n",
        "# Final window width\n",
        "final_window_width = window_width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgIKvsCq0C_S",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**2. Run this cell** to setup the animation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT10Bg9c0C_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# call the animator. blit=True means only re-draw the parts that have changed.\n",
        "anim_w_pred = FuncAnimation(\n",
        "    fig, animate_w_prediction,\n",
        "    frames=round((antenna_0.shape[0] - plot_width) / 100), blit=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pshOxtk0C_U",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**3. Run this cell** to build the animation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "FZhrI1yN0C_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Animate\n",
        "HTML(anim_w_pred.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQU1qlrz0C_W",
        "colab_type": "text"
      },
      "source": [
        "# Wrap-up\n",
        "\n",
        "Thank you for attending our masterclass! We hope we _demystified_ a little bit of what actually occurs when you build a machine learning process. Big thank you to Martin Oliveiro, Monica Munoz Castillo and Jackie Archer from Vodafone for lending us their time.\n",
        "\n",
        "If you would like to save your work for today, make sure to **save a copy of the notebook in Google Drive**.\n",
        "\n",
        "## Survey\n",
        "\n",
        "We would appreciate it if you could complete a quick feedback survey for tonight's Masterclass. You can find the survey here: http://bit.ly/ml_vodafone_survey\n",
        "\n",
        "THANK YOU!!"
      ]
    }
  ]
}